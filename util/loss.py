import torch
import torch.nn.functional as F
from einops import rearrange

def sparse_loss(router_probs: torch.Tensor) -> float:
    # num_groups, tokens_per_group, _ = router_probs.shape
    log_router_probs = torch.log(router_probs + 1e-10) 
    
    sparse_loss = torch.mean(router_probs * (log_router_probs + 1e-10), dim =-1)
    # return torch.sum(win_loss) / (num_groups * tokens_per_group)
    return torch.mean(sparse_loss)


def win_balance_loss(router_probs: torch.Tensor) -> float:
    # p = router_probs.mean(dim=0).mean(dim=0) + 1e-10
    # return torch.mean(p * (p.log() + 1e-10))
    num_experts = router_probs.shape[-1]
    p = router_probs.mean(dim=0) + 1e-10
    return torch.mean(p * (p.log() + 1e-10)) * num_experts


def node_balance_loss(router_probs: torch.Tensor) -> float:
    # probs shape b*t*n*num_experts
    num_experts = router_probs.shape[-1]
    p = router_probs.mean(dim=2) + 1e-10
    return torch.mean(p * (p.log() + 1e-10)) * num_experts


def router_z_loss_func(router_logits: torch.Tensor) -> float:
    r"""
    Compute the router z-loss implemented in PyTorch.

    The router z-loss was introduced in [Designing Effective Sparse Expert Models](https://arxiv.org/abs/2202.08906).
    It encourages router logits to remain small in an effort to improve stability.

    Args:
        router_logits (`float`):
            Input logits of shape [batch_size, sequence_length, num_experts]

    Returns:
        Scalar router z-loss.
    """
    num_groups, tokens_per_group, _ = router_logits.shape
    log_z = torch.logsumexp(router_logits, dim=-1)
    z_loss = log_z**2
    return torch.sum(z_loss) / (num_groups * tokens_per_group)


def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.Tensor) -> float:
    r"""
    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.

    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss
    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between
    experts is too unbalanced.

    Args:
        router_probs (`torch.Tensor`):
            Probability assigned to each expert per token. Shape: [batch_size, seqeunce_length, num_experts].
        expert_indices (`torch.Tensor`):
            Indices tensor of shape [batch_size, seqeunce_length] identifying the selected expert for a given token.

    Returns:
        The auxiliary loss.
    """
    num_experts = router_probs.shape[-1]

    # cast the expert indices to int64, otherwise one-hot encoding will fail
    if expert_indices.dtype != torch.int64:
        expert_indices = expert_indices.to(torch.int64)

    if len(expert_indices.shape) == 2:
        expert_indices = expert_indices.unsqueeze(2)

    expert_mask = torch.nn.functional.one_hot(expert_indices, num_experts)

    # For a given token, determine if it was routed to a given expert.
    expert_mask = torch.max(expert_mask, axis=-2).values

    # cast to float32 otherwise mean will fail
    expert_mask = expert_mask.to(torch.float32)
    tokens_per_group_and_expert = torch.mean(expert_mask, axis=-2)

    router_prob_per_group_and_expert = torch.mean(router_probs, axis=-2)
    return torch.mean(tokens_per_group_and_expert * router_prob_per_group_and_expert) * (num_experts**2)
